  # Slide 1 — Introduction (Person A)
## Title: AI Safety & Governance: Why It Matters
### Points:
-	AI gets more powerful every year — safety becomes a key concern.
-	2025 safety report: no major AI company fully met global safety expectations.
**Our presentation:**
o	What the report found
o	Why it matters
o	What can be done
**Discussion question at the end**
________________________________________
 # Slide 2 — Key Findings   (Person B)
## Title: Key Findings from 2025 AI Safety Report
### Points:
-	Major AI companies still have big safety-governance gaps.
-	No clear definitions of safe vs unsafe AI behaviour.
-	Weak internal safety rules (e.g., when development should pause).
-	Low transparency — little public info about testing or risk evaluation.
-	Insider reports: some models released without proper oversight.
-	Industry is moving fast, but not always carefully.
________________________________________
 # Slide 3 — Why This Matters  (Person C)
## Title: Why AI Safety Is Important
### Points:
-	Advanced AI can behave unpredictably or be misused.
-	Risks: bias, misinformation, privacy violations.
-	Some researchers warn about potential loss of control in very advanced systems.
-	Public trust can collapse after one major mistake.
-	Technology improves faster than safety practices.
________________________________________
# Slide 4 — What Should Be Done  (Person D)
## Title: Improving AI Safety
### Points:
-	Stronger government rules before releasing powerful models.
-	Independent safety audits + public risk reports.
-	Clear capability thresholds requiring expert review.
-	International cooperation — global issue, global impact.
________________________________________
# Slide 5 — Discussion Question (Person D)
## Title: Discussion Question
### Question:
- Should governments slow down the release of new AI models until they are proven safe, or should companies be allowed to move quickly and regulate themselves?
